{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic Language Modeling Comparison\n",
    "This notebook implements and compares three different architectures for Arabic language modeling:\n",
    "- N-gram model (traditional statistical approach)\n",
    "- LSTM-based RNN\n",
    "- Transformer with Flash Attention\n",
    "- Mamba SSM\n",
    "\n",
    "The models are trained on Arabic text data and evaluated using perplexity metrics. All models use the same BERT tokenizer, word embeddings and close parameters sizes for fair comparison.\n",
    "\n",
    "## Setup and Configuration\n",
    "Key configurations:\n",
    "- Context size: 4\n",
    "- Embedding dimension: 512\n",
    "- Sequence length: 64\n",
    "- Batch size: 256\n",
    "- Training split: 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tokenizers import BertWordPieceTokenizer, normalizers\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import pickle\n",
    "\n",
    "\n",
    "np.random.seed(seed=1)\n",
    "torch.manual_seed(1)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "CONTEXT_SIZE = 4\n",
    "EMBEDDING_DIM = 512\n",
    "files = [name for name in glob.glob('data/archive (2)/arwiki_books_shards/content/sharded/*.txt')] \n",
    "np.random.shuffle(files)\n",
    "vocab_path = 'data/archive (2)/ar_bert32k-vocab.txt'\n",
    "tokenizer = BertWordPieceTokenizer(vocab_path,strip_accents=False)\n",
    "max_length=2*CONTEXT_SIZE+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. N-gram Model Implementation\n",
    "Traditional N-gram model implementation with:\n",
    "- N = 2 (bigram model)\n",
    "- Basic smoothing for unseen sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "import tqdm\n",
    "import string\n",
    "\n",
    "\n",
    "n = 2\n",
    "\n",
    "def process_files(files):\n",
    "    tokens = []\n",
    "    translation_table = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    for file_path in tqdm.tqdm(files):\n",
    "        with open(file_path, 'r') as content_file:\n",
    "            text = content_file.read().translate(translation_table)\n",
    "            tokens.extend(tokenizer.encode(text).ids)\n",
    "\n",
    "    print('Done')\n",
    "    return tokens\n",
    "\n",
    "def build_ngram_model(tokens, n=n):\n",
    "    n_grams = ngrams(tokens, n, pad_right=True)\n",
    "    return FreqDist(n_grams)\n",
    "\n",
    "def predict_next_word(model, context, num_suggestions=1):\n",
    "    context = context.translate(str.maketrans('', '', string.punctuation))\n",
    "    context_tokens = tokenizer.encode(context, add_special_tokens=False).ids\n",
    "    \n",
    "    context_ngrams = {ngram[-1]: count for ngram, count in model.items() if ngram[:-1] == tuple(context_tokens)}\n",
    "    sorted_ngrams = sorted(context_ngrams.items(), key=lambda item: item[1], reverse=True)\n",
    "    return [tokenizer.id_to_token(word) for word, _ in sorted_ngrams[:num_suggestions]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=int(len(files)*0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = process_files(files[:train_size])\n",
    "n_gram_model = build_ngram_model(train_tokens, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save model\n",
    "if not os.path.exists('ngram.pkl'):\n",
    "        with open('ngram.pkl', 'wb') as f:\n",
    "                pickle.dump(n_gram_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ngram.pkl', 'rb') as f:\n",
    "    n_gram_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47823068"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_gram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['والسنة', '،', 'من', 'في', 'المقدس', 'الذي', 'على', 'لا', 'إلا', 'والحكمة']\n"
     ]
    }
   ],
   "source": [
    "start_sequence ='الكتاب'\n",
    "predicted_words = predict_next_word(n_gram_model, start_sequence, num_suggestions=10)\n",
    "print(predicted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity_ngram(model, test_tokens, n=2):\n",
    "    n_grams =list(ngrams(test_tokens, n, pad_right=True))\n",
    "    total_log_prob = 0\n",
    "    for n_gram in n_grams:\n",
    "        context = n_gram[:-1]\n",
    "        word = n_gram[-1]\n",
    "        if context in model:\n",
    "            context_prob = model[context].freq(word) / model[context].N()\n",
    "        else:\n",
    "            context_prob = 1 / model.N()  # Smoothing\n",
    "        total_log_prob += np.log(context_prob)\n",
    "    \n",
    "    perplexity = np.exp(-total_log_prob / len(n_grams))\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [05:40<00:00,  6.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "N-Gram Model Perplexity: 690866710.0103099\n"
     ]
    }
   ],
   "source": [
    "test_tokens = process_files(files[train_size:])\n",
    "ngram_perplexity = calculate_perplexity_ngram(n_gram_model, test_tokens, n=n)\n",
    "print(f'N-Gram Model Perplexity: {ngram_perplexity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM-based RNN Model\n",
    "Implementation details:\n",
    "- 7-layer LSTM architecture\n",
    "- Shared embedding layer with initial word2vec weights\n",
    "- Uses AdamW optimizer with weight decay\n",
    "- Linear learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(f'checkpoints/word2vec_{EMBEDDING_DIM}_c{CONTEXT_SIZE}_n{CONTEXT_SIZE}_cobw.bin')\n",
    "not_train= []\n",
    "embedding=nn.Embedding(tokenizer.get_vocab_size(),EMBEDDING_DIM)\n",
    "for word, idx in tokenizer.get_vocab().items():\n",
    "    if model.has_index_for(word):\n",
    "        with torch.no_grad():\n",
    "            embedding.weight[idx]=torch.tensor(model.get_vector(word))\n",
    "            not_train.append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def criterion(output,true_t):\n",
    "#     return 1- torch.mean(F.cosine_similarity(output,embedding(true_t)))  \n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_streaming_dataloaders\n",
    "\n",
    "max_length = 64\n",
    "batch_size = 256\n",
    "batch_buffer_size = 100\n",
    "stride = 32\n",
    "train_size=int(len(files)*0.9)\n",
    "trainloader = create_streaming_dataloaders(\n",
    "    files=files[:train_size],\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size,  \n",
    "    sequence_length=max_length,  \n",
    "    stride=stride,  \n",
    "    batch_buffer_size=batch_buffer_size,\n",
    "    shuffle_buffer=True,\n",
    ")\n",
    "\n",
    "validloader = create_streaming_dataloaders(\n",
    "    files=files[train_size:],\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size,  \n",
    "    sequence_length=max_length,  \n",
    "    stride=max_length,  \n",
    "    batch_buffer_size=batch_buffer_size,\n",
    "    shuffle_buffer= False\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_layers=7\n",
    "\n",
    "# embedding=nn.Embedding(tokenizer.get_vocab_size(),EMBEDDING_DIM)\n",
    "# embedding.load_state_dict(torch.load('gramEmbed.pt'))\n",
    "# embedding.weight.requires_grad=False\n",
    "class StarLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(StarLSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # self.lstm2 = nn.LSTM( hidden_size, input_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        # h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        o, (hn, cn) = self.lstm1(x)\n",
    "        # print(h[0].shape)\n",
    "        # o, (hn, cn) = self.lstm2(hn[-1], (h0, c0))\n",
    "   \n",
    "        return o\n",
    "\n",
    "lstm=StarLSTM(EMBEDDING_DIM, EMBEDDING_DIM, num_layers=num_layers)\n",
    "#lstm.load_state_dict(torch.load(f'best_lstm{num_layers}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parameters : 14,708,736'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"parameters : {sum(p.numel() for p in lstm.parameters() if p.requires_grad):,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32000, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs=6\n",
    "learning_rate =0.001  \n",
    "weight_decay = 0.002     \n",
    "# momentum=0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "optimizer = torch.optim.AdamW( [{'params':lstm.parameters(),'lr': learning_rate}, {'params':embedding.parameters(), 'lr': learning_rate*0.3}] ,weight_decay=weight_decay,)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer,start_factor=1, end_factor=0.1, total_iters=total_epochs)\n",
    "# torch.set_float32_matmul_precision('high')\n",
    "# torch.backends.cuda.matmul.allow_tf32=True\n",
    "# torch.backends.cudnn.allow_tf32=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm=lstm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, starting fresh training\n"
     ]
    }
   ],
   "source": [
    "from utils import train, setup_training_state\n",
    "lstm = torch.compile(lstm)\n",
    "lstm, embedding, start_epoch = setup_training_state(\n",
    "        model=lstm,\n",
    "        embedding=embedding,\n",
    "        checkpoint_dir='checkpoints',\n",
    "        num_layers=num_layers,\n",
    "        prefix='LSTMembedAslinear'\n",
    "    )\n",
    "# load_model_and_embedding(lstm, embedding, prefix='embedAslinear', num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.1930: : 86888it [1:59:24, 12.13it/s]\n",
      "Validating: 5246it [03:42, 23.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 | train loss:5.1146 | valid loss:0.0193 | lr: [0.001, 0.0003]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 5.0016: : 86888it [1:59:00, 12.17it/s]\n",
      "Validating: 5246it [03:42, 23.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2 | train loss:4.7566 | valid loss:0.0187 | lr: [0.00085, 0.00025499999999999996]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.9578: : 86888it [1:58:33, 12.21it/s]\n",
      "Validating: 5246it [03:44, 23.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3 | train loss:4.6515 | valid loss:0.0184 | lr: [0.0006999999999999999, 0.00020999999999999995]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.8908: : 86888it [1:58:29, 12.22it/s]\n",
      "Validating: 5246it [03:41, 23.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4 | train loss:4.5852 | valid loss:0.0182 | lr: [0.0005499999999999999, 0.00016499999999999997]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.8507: : 86888it [1:58:21, 12.24it/s]\n",
      "Validating: 5246it [03:41, 23.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5 | train loss:4.5354 | valid loss:0.0180 | lr: [0.00039999999999999996, 0.00011999999999999999]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 4.8502: : 86888it [1:58:27, 12.22it/s]\n",
      "Validating: 5246it [03:42, 23.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:6 | train loss:4.4946 | valid loss:0.0178 | lr: [0.00025, 7.5e-05]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_losses, valid_losses=train(lstm, embedding, optimizer, scheduler, total_epochs - start_epoch, trainloader, validloader,criterion,num_layers, use_amp=True, prefix='LSTMembedAslinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Model Perplexity: 95.08717125843353\n"
     ]
    }
   ],
   "source": [
    "from utils import calculate_perplexity\n",
    "rnn_perplexity = calculate_perplexity(lstm,embedding, validloader, device)\n",
    "print(f'RNN Model Perplexity: {rnn_perplexity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: ['هذا الكتاب ، ولم يزل كذلك إلى أن مات. قال ابن الجوزي كان من أهل الأدب ، كثير التواضع وحسن الخلق']\n"
     ]
    }
   ],
   "source": [
    "from utils import sample_from_model\n",
    "start_sequence = 'هذا الكتاب'\n",
    "generated_text = sample_from_model(lstm,embedding,tokenizer,start_sequence,top_k=10,num_generate=20)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformer Model\n",
    "Key features:\n",
    "- Flash Attention for efficiency\n",
    "- 6 transformer layers\n",
    "- Feed-forward network with expansion ratio 2.66\n",
    "- Dropout rate 0.1\n",
    "- Shared embedding layer with learnable positional encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(f'checkpoints/word2vec_{EMBEDDING_DIM}_c{CONTEXT_SIZE}_n{CONTEXT_SIZE}_cobw.bin')\n",
    "not_train= []\n",
    "embedding=nn.Embedding(tokenizer.get_vocab_size(),EMBEDDING_DIM)\n",
    "for word, idx in tokenizer.get_vocab().items():\n",
    "    if model.has_index_for(word):\n",
    "        with torch.no_grad():\n",
    "            embedding.weight[idx]=torch.tensor(model.get_vector(word))\n",
    "            not_train.append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def criterion(output,true_t):\n",
    "#     return 1- torch.mean(F.cosine_similarity(output,embedding(true_t)))  \n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_streaming_dataloaders\n",
    "\n",
    "max_length = 64\n",
    "batch_size = 256\n",
    "batch_buffer_size = 100\n",
    "stride = 32\n",
    "train_size=int(len(files)*0.9)\n",
    "trainloader = create_streaming_dataloaders(\n",
    "    files=files[:train_size],\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size,  \n",
    "    sequence_length=max_length,  \n",
    "    stride=stride,  \n",
    "    batch_buffer_size=batch_buffer_size,\n",
    "    num_workers=8,\n",
    "    shuffle_buffer=True,\n",
    ")\n",
    "\n",
    "validloader = create_streaming_dataloaders(\n",
    "    files=files[train_size:],\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size,  \n",
    "    sequence_length=max_length,  \n",
    "    stride=max_length,  \n",
    "    batch_buffer_size=batch_buffer_size,\n",
    "    num_workers=4,\n",
    "    shuffle_buffer= False,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "\n",
    "class FlashAttention(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask = None):\n",
    "        batch_size, seq_len, dim = x.shape\n",
    "        \n",
    "        # reshaping directly without transposing would result in incorrect tensor dimensions will not reshape on dim // num_heads\n",
    "        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        \n",
    "        # Flash attention implementation\n",
    "        with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "            attn_output = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                attn_mask=mask,\n",
    "                dropout_p=self.dropout if self.training else 0.0,\n",
    "                is_causal=mask is None\n",
    "            )\n",
    "        \n",
    "        # Reshape output .contiguous()\n",
    "        attn_output = attn_output.transpose(1, 2).view(batch_size, seq_len, self.dim)\n",
    "        return self.out_proj(attn_output)\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, mlp_ratio: float = 2.0, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = FlashAttention(dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.ff = FeedForward(dim, int(dim * mlp_ratio), dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask = None):\n",
    "        x = x + self.attn(self.norm1(x), mask)\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int = 512,\n",
    "        depth: int = 6,\n",
    "        num_heads: int = 8,\n",
    "        mlp_ratio: float = 2.5,\n",
    "        dropout: float = 0.1,\n",
    "        max_seq_length: int = 1024,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.position_embedding = nn.Parameter(torch.zeros(1, max_seq_length, dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask = None):\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        x = x + self.position_embedding[:, :seq_len]\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "            \n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth =6\n",
    "model = DecoderTransformer(dim=EMBEDDING_DIM, depth=depth, mlp_ratio=2.66, max_seq_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parameters : 14,723,046'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"parameters : {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs=6\n",
    "learning_rate =0.0006999999999999999 # to complete from the0.0006999999999999999 last iter \n",
    "weight_decay = 0.002     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "optimizer = torch.optim.AdamW( [{'params':model.parameters()}, {'params':embedding.parameters(), 'lr': learning_rate*0.2}] # 0.2 because 0.3 caused nan values\n",
    "                               , lr=learning_rate,weight_decay=weight_decay,)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer,start_factor=1, end_factor=0.1, total_iters=total_epochs)\n",
    "# torch.set_float32_matmul_precision('high')\n",
    "# torch.backends.cuda.matmul.allow_tf32=True\n",
    "# torch.backends.cudnn.allow_tf32=True\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from checkpoints/TraEmbedAslinearlast_checkpoint6.pt\n",
      "Successfully loaded checkpoint from epoch 3\n",
      "Resumed training from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naif/projects/next_word_prediction/utils.py:435: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "from utils import train, setup_training_state\n",
    "model = torch.compile(model)\n",
    "model, embedding, start_epoch = setup_training_state(\n",
    "        model=model,\n",
    "        embedding=embedding,\n",
    "        checkpoint_dir='checkpoints',\n",
    "        num_layers=depth,\n",
    "        prefix='TraEmbedAslinear'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.4562: : 56636it [1:32:08, 12.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip: batch{batch_idx} for nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.8792: : 86887it [2:21:27, 10.24it/s]\n",
      "Validating: 5244it [04:41, 18.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 | train loss:4.7446 | valid loss:0.0181 | lr: [0.0006999999999999999, 0.00014]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.7970: : 86887it [2:21:11, 10.26it/s]\n",
      "Validating: 5244it [04:32, 19.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2 | train loss:4.6831 | valid loss:0.0179 | lr: [0.0005949999999999999, 0.00011899999999999999]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.7491: : 86887it [2:21:28, 10.24it/s]\n",
      "Validating: 5244it [04:32, 19.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3 | train loss:4.6402 | valid loss:0.0178 | lr: [0.0004899999999999999, 9.799999999999998e-05]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_losses,valid_losses=train(model, embedding, optimizer, scheduler, total_epochs - start_epoch, trainloader, validloader,criterion,depth, use_amp=True, prefix='TraEmbedAslinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Model Perplexity: 93.86618531236887\n"
     ]
    }
   ],
   "source": [
    "from utils import calculate_perplexity\n",
    "tran_perplexity = calculate_perplexity(model,embedding, validloader, device)\n",
    "print(f'Transformer Model Perplexity: {tran_perplexity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: ['هذا الكتاب من عند الله وهو كتاب أنزله الله على نبيه محمد صلى الله عليه و سلم قال أبو بكر حدثنا محمد']\n"
     ]
    }
   ],
   "source": [
    "from utils import sample_from_model\n",
    "start_sequence = 'هذا الكتاب'\n",
    "generated_text = sample_from_model(model,embedding,tokenizer,start_sequence,top_k=10,num_generate=20)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mamba SSM Model\n",
    "Implementation using Mamba State Space Model:\n",
    "- 6 Mamba layers\n",
    "- State dimension: 64\n",
    "- Convolution width: 4\n",
    "- Expansion factor: 3\n",
    "- Scaled embedding with sqrt(dim) * 2\n",
    "\n",
    "Training Stability Notes:\n",
    "- Learning rate reduced to 1e-4 for stability\n",
    "- Embedding scaling factor (sqrt(dim) * 2) crucial for preventing NaN losses\n",
    "- Embedding learning rate set to main_lr * 0.2\n",
    "- These adjustments were necessary to prevent training instability and NaN values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(f'checkpoints/word2vec_{EMBEDDING_DIM}_c{CONTEXT_SIZE}_n{CONTEXT_SIZE}_cobw.bin')\n",
    "not_train= []\n",
    "embedding=nn.Embedding(tokenizer.get_vocab_size(),EMBEDDING_DIM)\n",
    "for word, idx in tokenizer.get_vocab().items():\n",
    "    if model.has_index_for(word):\n",
    "        with torch.no_grad():\n",
    "            embedding.weight[idx]=torch.tensor(model.get_vector(word))\n",
    "            not_train.append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def criterion(output,true_t):\n",
    "#     return 1- torch.mean(F.cosine_similarity(output,embedding(true_t)))  \n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_streaming_dataloaders\n",
    "\n",
    "max_length = 64\n",
    "batch_size = 256\n",
    "batch_buffer_size = 100\n",
    "stride = 32\n",
    "train_size=int(len(files)*0.9)\n",
    "trainloader = create_streaming_dataloaders(\n",
    "    files=files[:train_size],\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size,  \n",
    "    sequence_length=max_length,  \n",
    "    stride=stride,  \n",
    "    batch_buffer_size=batch_buffer_size,\n",
    "    num_workers=0,\n",
    "    shuffle_buffer=True,\n",
    ")\n",
    "\n",
    "validloader = create_streaming_dataloaders(\n",
    "    files=files[train_size:],\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size,  \n",
    "    sequence_length=max_length,  \n",
    "    stride=max_length,  \n",
    "    batch_buffer_size=batch_buffer_size,\n",
    "    num_workers=0,\n",
    "    shuffle_buffer= False,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm import Mamba2\n",
    "\n",
    "\n",
    "class MammbaModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int = 512,\n",
    "        depth: int = 6,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed_scale = np.sqrt(dim) * 2\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            Mamba2(\n",
    "            # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "            d_model=dim, # Model dimension d_model\n",
    "            d_state=64,  # SSM state expansion factor, typically 64 or 128\n",
    "            d_conv=4,    # Local convolution width\n",
    "            expand=3,    # Block expansion factor\n",
    "            # dt_limit=0,\n",
    "        )\n",
    "        for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.dropout(x / self.embed_scale)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth =6\n",
    "model = MammbaModel(dim=EMBEDDING_DIM, depth=depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parameters : 14,683,312'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"parameters : {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs=6\n",
    "learning_rate =1e-4  # to complete from the last iter \n",
    "weight_decay = 0.002     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "optimizer = torch.optim.AdamW( [{'params':model.parameters()}, {'params':embedding.parameters(), 'lr': learning_rate*0.2}] \n",
    "                               , lr=learning_rate,weight_decay=weight_decay,)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer,start_factor=1, end_factor=0.1, total_iters=total_epochs)\n",
    "# torch.set_float32_matmul_precision('high')\n",
    "# torch.backends.cuda.matmul.allow_tf32=True\n",
    "# torch.backends.cudnn.allow_tf32=True\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, starting fresh training\n"
     ]
    }
   ],
   "source": [
    "from utils import train, setup_training_state\n",
    "# model = torch.compile(model)\n",
    "model, embedding, start_epoch = setup_training_state(\n",
    "        model=model,\n",
    "        embedding=embedding,\n",
    "        checkpoint_dir='checkpoints',\n",
    "        num_layers=depth,\n",
    "        prefix='MambaEmbedAslinear'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.1022: : 86833it [3:24:19,  7.08it/s]\n",
      "Validating: 5237it [05:15, 16.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 | train loss:5.4972 | valid loss:0.0191 | lr: [0.0001, 2e-05]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.9680: : 86833it [3:27:04,  6.99it/s]\n",
      "Validating: 5237it [05:15, 16.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2 | train loss:4.8150 | valid loss:0.0185 | lr: [8.5e-05, 1.7e-05]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 4.8196: : 86833it [3:27:37,  6.97it/s]\n",
      "Validating: 5237it [05:15, 16.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3 | train loss:4.7007 | valid loss:0.0182 | lr: [7.000000000000001e-05, 1.4e-05]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 4.8320: : 86833it [3:28:25,  6.94it/s]\n",
      "Validating: 5237it [05:16, 16.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4 | train loss:4.6360 | valid loss:0.0180 | lr: [5.5e-05, 1.1e-05]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 4.7473: : 86833it [3:28:18,  6.95it/s]\n",
      "Validating: 5237it [05:16, 16.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5 | train loss:4.5923 | valid loss:0.0179 | lr: [4e-05, 8e-06]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 4.7137: : 86833it [3:28:16,  6.95it/s]\n",
      "Validating: 5237it [05:17, 16.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:6 | train loss:4.5600 | valid loss:0.0178 | lr: [2.5e-05, 4.9999999999999996e-06]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_losses,valid_losses=train(model, embedding, optimizer, scheduler, total_epochs - start_epoch, trainloader, validloader,criterion,depth, use_amp=True, prefix='MambaEmbedAslinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Model Perplexity: 94.16253863516155\n"
     ]
    }
   ],
   "source": [
    "from utils import calculate_perplexity\n",
    "tran_perplexity = calculate_perplexity(model,embedding, validloader, device)\n",
    "print(f'Transformer Model Perplexity: {tran_perplexity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: ['هذا الكتاب ، وهذا ما حدث في عهد الملك الناصر فرج في عهد السلطان صلاح الدين الأيوبي ، حيث كان قد']\n"
     ]
    }
   ],
   "source": [
    "from utils import sample_from_model\n",
    "start_sequence = 'هذا الكتاب'\n",
    "generated_text = sample_from_model(model,embedding,tokenizer,start_sequence,top_k=10,num_generate=20)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Challenges:\n",
    "1. Threading and Tokenization:\n",
    "   - Issues encountered with parallel file processing and batch encoding\n",
    "   - System became stable by setting num_workers=0 and enabling TOKENIZERS_PARALLELISM instead of the file parallelism method\n",
    "   - Possible memory/thread contention issues with encode_batch method\n",
    "\n",
    "2. Training Observations:\n",
    "   - Models trained for 6 epochs with stride=32 (effective 12 epochs)\n",
    "   - Transformer showed more natural text generation\n",
    "   - RNN and Mamba occasionally produced repetitive punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 502325,
     "sourceId": 954009,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
